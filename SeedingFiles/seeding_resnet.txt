# lines that start with "#" are ignored and can be used as comments
# resnet impelementation based on https://github.com/priya-dwivedi/Deep-Learning/blob/master/resnet_keras/Residual_Networks_yourself.ipynb

# the resnet individual -- this is the exact model from above (aside from changes made to mod_21 and the lack of padding at the start). It fails due to negative dimension size
# NNLearner(ARG0, PassLayerList(FlattenLayer(mod_30(mod_25(mod_29(mod_24(mod_24(mod_24(mod_24(mod_24(mod_28(mod_23(mod_23(mod_23(mod_27(mod_22(mod_22(mod_26(mod_21(Input(ARG1)))))))))))))))))))), AdamOptimizer, ImageAugmentation(data_dims, FlipHorizontal, NoRotation, NoZoom, NoTranslation))

# This is the same as above but omits avg pooling at the end to avoid the error. Does not do well because the first mod reduces data to 7x7 which seems to be too much
# NNLearner(ARG0, PassLayerList(FlattenLayer(mod_25(mod_25(mod_29(mod_24(mod_24(mod_24(mod_24(mod_24(mod_28(mod_23(mod_23(mod_23(mod_27(mod_22(mod_22(mod_26(mod_21(Input(ARG1)))))))))))))))))))), AdamOptimizer, ImageAugmentation(data_dims, FlipHorizontal, NoRotation, NoZoom, NoTranslation))

# new attempt: avg pooling at the end but no max pooling at the start (mod_21 is adjusted)
NNLearner(ARG0, PassLayerList(FlattenLayer(mod_30(mod_25(mod_29(mod_24(mod_24(mod_24(mod_24(mod_24(mod_28(mod_23(mod_23(mod_23(mod_27(mod_22(mod_22(mod_26(mod_21(Input(ARG1)))))))))))))))))))), AdamOptimizer, ImageAugmentation(data_dims, FlipHorizontal, NoRotation, NoZoom, NoTranslation))
NNLearner(ARG0, PassLayerList(FlattenLayer(mod_30(mod_25(mod_29(mod_24(mod_24(mod_24(mod_24(mod_24(mod_28(mod_23(mod_23(mod_23(mod_27(mod_22(mod_22(mod_26(mod_21(Input(ARG1)))))))))))))))))))), AdamOptimizer, ImageAugmentation(data_dims, NoFlip, NoRotation, NoZoom, NoTranslation))

# this is the starter module; using 5x5 kernel instead of 7x7 because they are using 64x64 image dataset. Also using stride1 because we don't have larger ones implemented atm:
# mod_21: Module(datatype, data_dims, Conv2DLayer(LayerUnit32, reluActivation, KernelSize5, Stride2, ValidPadding, Batch_Normalization, ARG0), NoSkipConnection4dim, MaxPool2D(PoolSize2), NoDropout)
mod_21: Module(datatype, data_dims, Conv2DLayer(LayerUnit32, reluActivation, KernelSize5, Stride2, ValidPadding, Batch_Normalization, ARG0), NoSkipConnection4dim, NoPooling(), NoDropout)

# these are the identity blocks:
mod_22: Module(datatype, data_dims, Conv2DLayer(LayerUnit128, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit32, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit32, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)
mod_23: Module(datatype, data_dims, Conv2DLayer(LayerUnit256, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)
mod_24: Module(datatype, data_dims, Conv2DLayer(LayerUnit512, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)
mod_25: Module(datatype, data_dims, Conv2DLayer(LayerUnit1024, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)

# these are the convolutional blocks:
mod_26: Module(datatype, data_dims, Conv2DLayer(LayerUnit128, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit32, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit32, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)
mod_27: Module(datatype, data_dims, Conv2DLayer(LayerUnit256, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)
mod_28: Module(datatype, data_dims, Conv2DLayer(LayerUnit512, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)
mod_29: Module(datatype, data_dims, Conv2DLayer(LayerUnit1024, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)

# this is a special identity block with average pooling used at the end
mod_30: Module(datatype, data_dims, Conv2DLayer(LayerUnit1024, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), AvgPool2D(PoolSize2), NoDropout)




# new attempt: using original out dimensions
NNLearner(ARG0, PassLayerList(FlattenLayer(mod_20(mod_15(mod_19(mod_14(mod_14(mod_14(mod_14(mod_14(mod_18(mod_13(mod_13(mod_13(mod_17(mod_12(mod_12(mod_16(mod_11(Input(ARG1)))))))))))))))))))), AdamOptimizer, ImageAugmentation(data_dims, FlipHorizontal, NoRotation, NoZoom, NoTranslation))
NNLearner(ARG0, PassLayerList(FlattenLayer(mod_20(mod_15(mod_19(mod_14(mod_14(mod_14(mod_14(mod_14(mod_18(mod_13(mod_13(mod_13(mod_17(mod_12(mod_12(mod_16(mod_11(Input(ARG1)))))))))))))))))))), AdamOptimizer, ImageAugmentation(data_dims, NoFlip, NoRotation, NoZoom, NoTranslation))

# this is the starter module; using 5x5 kernel instead of 7x7 because they are using 64x64 image dataset. Also using stride1 because we don't have larger ones implemented atm:
mod_11: Module(datatype, data_dims, Conv2DLayer(LayerUnit64, reluActivation, KernelSize5, Stride2, ValidPadding, Batch_Normalization, ARG0), NoSkipConnection4dim, NoPooling(), NoDropout)

# these are the identity blocks:
mod_12: Module(datatype, data_dims, Conv2DLayer(LayerUnit256, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)
mod_13: Module(datatype, data_dims, Conv2DLayer(LayerUnit512, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)
mod_14: Module(datatype, data_dims, Conv2DLayer(LayerUnit1024, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)
mod_15: Module(datatype, data_dims, Conv2DLayer(LayerUnit2048, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit512, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit512, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), NoPooling(), NoDropout)

# these are the convolutional blocks:
mod_16: Module(datatype, data_dims, Conv2DLayer(LayerUnit256, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit64, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)
mod_17: Module(datatype, data_dims, Conv2DLayer(LayerUnit512, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit128, reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)
mod_18: Module(datatype, data_dims, Conv2DLayer(LayerUnit1024, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit256, reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)
mod_19: Module(datatype, data_dims, Conv2DLayer(LayerUnit2048, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit512, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit512, reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization, ARG0))), Conv2DConnection(reluActivation, KernelSize1, Stride2, ValidPadding, Batch_Normalization), NoPooling(), NoDropout)

# this is a special identity block with average pooling used at the end
mod_20: Module(datatype, data_dims, Conv2DLayer(LayerUnit2048, linearActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, Conv2DLayer(LayerUnit512, reluActivation, KernelSize3, Stride1, SamePadding, Batch_Normalization, Conv2DLayer(LayerUnit512, reluActivation, KernelSize1, Stride1, ValidPadding, Batch_Normalization, ARG0))), SkipConnection4dim(reluActivation), AvgPool2D(PoolSize2), NoDropout)